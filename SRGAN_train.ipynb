{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.models import vgg19\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 20\n",
    "batch_size = 16\n",
    "scale = 4\n",
    "patch_size = (96, 96)\n",
    "num_residual_blocks = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_patches(image, patch_size):\n",
    "    channels, height, width = image.size()\n",
    "    patch_height, patch_width = patch_size\n",
    "    patches = []\n",
    "    \n",
    "    # Calculate padding needed for width and height\n",
    "    pad_height = patch_height - height%patch_height\n",
    "    pad_width = patch_width - width%patch_width\n",
    "    \n",
    "    # Apply the padding\n",
    "    padded_image = F.pad(image, (0, pad_width, 0, pad_height))\n",
    "    \n",
    "    # start, stop, step\n",
    "    for y in range(0, height, patch_height):\n",
    "        for x in range(0, width, patch_width):\n",
    "            # the patch consist of all the channels, \n",
    "            patch = padded_image[:, y:y+patch_height, x:x+patch_width]\n",
    "            patches.append(patch)\n",
    "            \n",
    "            \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images_in_folder(folder_path, scale, patch_size):\n",
    "    \n",
    "    processed_images = []\n",
    "    \n",
    "    for filename in tqdm(os.listdir(folder_path), desc=\"Processing images\", colour='green') :\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        input_image = cv2.imread(image_path)\n",
    "        \n",
    "        input_image_rgb = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # change cv2 BGR to RGB\n",
    "        GT_tensor = torch.tensor(input_image_rgb, dtype=torch.float32).permute(2,0,1)/255\n",
    "        \n",
    "        \n",
    "        GT_image_pil = Image.fromarray(input_image_rgb)\n",
    "        LR_image = TF.resize(GT_image_pil, [GT_image_pil.size[1] // scale, GT_image_pil.size[0] // scale], interpolation=Image.BICUBIC)\n",
    "        \n",
    "        LR_tensor_resized = TF.resize(LR_image, [GT_tensor.shape[1], GT_tensor.shape[2]], interpolation=Image.BICUBIC)\n",
    "        LR_tensor_resized = TF.to_tensor(LR_tensor_resized).float()\n",
    "        \n",
    "        LR_patches = image_to_patches(LR_tensor_resized, patch_size)\n",
    "        GT_patches = image_to_patches(GT_tensor, patch_size)\n",
    "        processed_images.extend(zip(LR_patches, GT_patches))\n",
    "        \n",
    "    return processed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(LR, GT):\n",
    "    \n",
    "    # Display Ground Truth Image\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(GT.permute(1,2,0))\n",
    "    plt.title(\"Ground Truth (GT)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Display the input image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(LR.permute(1, 2, 0))\n",
    "    plt.title(\"Low Resolution (LR)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing training and test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_folder_path = 'data/Train/ILSVRC2013'\n",
    "test_images_set5_folder_path = 'data/Test/Set5'\n",
    "test_images_set14_folder_path = 'data/Test/Set14'\n",
    "\n",
    "processed_train_images = preprocess_images_in_folder(train_images_folder_path, scale, patch_size)\n",
    "processed_test_set5_images = preprocess_images_in_folder(test_images_set5_folder_path, scale, patch_size)\n",
    "processed_test_set14_images = preprocess_images_in_folder(test_images_set14_folder_path, scale, patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming processed_images is your list of LR and GT patches\n",
    "random_pair = random.choice(processed_test_set5_images)\n",
    "LR_patch, GT_patch = random_pair\n",
    "\n",
    "# Plot LR image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(LR_patch.permute(1, 2, 0))  # Permute LR image dimensions to (height, width, channels) and plot\n",
    "plt.title('Low-Resolution (LR) Image')\n",
    "plt.axis('off')\n",
    "\n",
    "print(LR_patch.size())\n",
    "# Plot GT image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(GT_patch.permute(1, 2, 0))  # Assuming GT image is also in (height, width, channels) format\n",
    "plt.title('Ground Truth (GT) Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting images in DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(processed_train_images, batch_size=batch_size)\n",
    "test_set5_loader = DataLoader(processed_test_set5_images, batch_size=batch_size)\n",
    "test_set14_loader = DataLoader(processed_test_set14_images, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Residual Block classes for Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlockGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResBlockGenerator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding='same')\n",
    "        self.batch_norm1 = nn.BatchNorm2d(64, momentum=0.5)\n",
    "        self.p_relu = nn.PReLU()\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding='same')\n",
    "        self.batch_norm2 = nn.BatchNorm2d(64, momentum=0.5)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        out = self.conv1(input_tensor)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.p_relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        out += input_tensor\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_residual_blocks = num_residual_blocks):\n",
    "        super(Generator, self).__init__()\n",
    "        # Pre residual layers\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, stride=1, padding='same')\n",
    "        self.p_relu1 = nn.PReLU()\n",
    "        \n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResBlockGenerator() for _ in range(num_residual_blocks)]\n",
    "        )\n",
    "        \n",
    "        # Post residual layers\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding='same')\n",
    "        self.batch_norm = nn.BatchNorm2d(64, momentum=0.5)\n",
    "        \n",
    "        # Upsampling layers\n",
    "        self.upsample1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        self.upsample2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        # Output convolutional layer\n",
    "        self.conv3 = nn.Conv2d(64, 3, kernel_size=9, stride=1, padding='same')\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        out = self.conv1(input_tensor)\n",
    "        out = self.p_relu1(out)\n",
    "        input_residual_tensor = out.clone()\n",
    "        \n",
    "        for res_block in self.residual_blocks:\n",
    "            out = res_block(out)\n",
    "            \n",
    "        out = self.conv2(out)\n",
    "        out = self.batch_norm(out)\n",
    "        out += input_residual_tensor\n",
    "        \n",
    "        out = self.upsample1(out)\n",
    "        out = self.upsample2(out)\n",
    "            \n",
    "        out = self.conv3(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorBlock(nn.Module):\n",
    "    def __init__(self, in_channel,out_channel, strides = 1, padding_same=True):\n",
    "        super(DiscriminatorBlock, self).__init__()\n",
    "        \n",
    "        if padding_same:\n",
    "            self.padding = 'same'\n",
    "        else:\n",
    "            self.padding = 0\n",
    "            \n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=strides, padding=self.padding)\n",
    "        self.batch_normalization = nn.BatchNorm2d(out_channel, momentum=0.8)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        out = self.conv(input_tensor)\n",
    "        out = self.batch_normalization(out)\n",
    "        out = self.leaky_relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding='same')\n",
    "        self.leaky_relu1 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.block_with_batch_norm = nn.Sequential(\n",
    "            DiscriminatorBlock(64, 64, strides=2, padding_same=False),\n",
    "            DiscriminatorBlock(64, 128),\n",
    "            DiscriminatorBlock(128, 128,strides=2, padding_same=False),\n",
    "            DiscriminatorBlock(128, 256),\n",
    "            DiscriminatorBlock(256, 256, strides=2, padding_same=False),\n",
    "            DiscriminatorBlock(256, 512, ),\n",
    "            DiscriminatorBlock(512, 512, strides=2, padding_same=False)\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(512 * 5 * 5, 1024)\n",
    "        self.leaky_relu2 = nn.LeakyReLU(0.2)\n",
    "        self.dense2 = nn.Linear(1024, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        out = self.conv1(input_tensor)\n",
    "        out = self.leaky_relu1(out)\n",
    "        out = self.block_with_batch_norm(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.dense1(out)\n",
    "        out = self.leaky_relu2(out)\n",
    "        out = self.dense2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating VGG loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phi_2,2, after 2nd activation, but before the 2nd max pooling\n",
    "\n",
    "class VGG19Loss(nn.Module):\n",
    "    def __init__(self, layer_index=9):\n",
    "        super(VGG19Loss, self).__init__()\n",
    "        vgg19_model = vgg19(weights='DEFAULT')\n",
    "        \n",
    "        # vgg19_model.features.children() returns an iterator over each individual layer\n",
    "        # list() converts the iterator into a Python list\n",
    "        # [:layer_index + 1] slices the list up to and including the layer at layer_index\n",
    "        # * is the unpacking operator, which makes the list elements suitable to be passed to nn.Sequential \n",
    "        self.features = nn.Sequential(*list(vgg19_model.features.children())[:layer_index + 1])\n",
    "        \n",
    "        # Make sure no updating of the parameters occur\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, sr, hr):\n",
    "        sr_features = self.features(sr)\n",
    "        hr_features = self.features(hr)\n",
    "        \n",
    "        loss = self.criterion(sr_features, hr_features)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining generator, discriminator and VGG loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(num_residual_blocks=16)\n",
    "discriminator = Discriminator()\n",
    "vgg_loss = VGG19Loss()\n",
    "\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "vgg_loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining optimizers and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_generator = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
    "\n",
    "adversarial_criterion = nn.BCELoss()\n",
    "pixel_criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch}/{num_epochs}', position=0, leave=True) as pbar_epoch:\n",
    "        for i, (imgs_lr, imgs_hr) in enumerate(train_loader):\n",
    "            \n",
    "            imgs_lr = imgs_lr.to(device)\n",
    "            imgs_hr = imgs_hr.to(device)\n",
    "        \n",
    "            # Training the generator\n",
    "            fake_hr = generator(imgs_lr)\n",
    "            fake_hr_resized = F.interpolate(fake_hr, size=(96, 96), mode='bilinear', align_corners=False)\n",
    "        \n",
    "            optimizer_generator.zero_grad()\n",
    "        \n",
    "            real_labels = torch.ones((imgs_hr.size(0), 1), requires_grad=False).to(device)\n",
    "            fake_labels = torch.zeros((imgs_hr.size(0), 1), requires_grad=False).to(device)\n",
    "        \n",
    "            # Training the discriminator with real images\n",
    "            outputs_real = discriminator(imgs_hr)\n",
    "            loss_real = adversarial_criterion(outputs_real, real_labels)\n",
    "        \n",
    "            # Training discriminator with fake images\n",
    "            outputs_fake = discriminator(fake_hr_resized.detach())\n",
    "            loss_fake = adversarial_criterion(outputs_fake, fake_labels)\n",
    "        \n",
    "            loss_discriminator = (loss_real + loss_fake)/2\n",
    "            loss_discriminator.backward()\n",
    "            optimizer_discriminator.step()\n",
    "        \n",
    "            optimizer_generator.zero_grad()\n",
    "        \n",
    "            outputs_fake = discriminator(fake_hr_resized)\n",
    "            adversarial_loss = adversarial_criterion(outputs_fake, real_labels)\n",
    "        \n",
    "            pixel_loss = pixel_criterion(fake_hr_resized, imgs_hr)\n",
    "        \n",
    "            perceptual_loss = vgg_loss(fake_hr_resized, imgs_hr)\n",
    "        \n",
    "            perceptual_adversarial_loss = perceptual_loss + 0.001 * adversarial_loss\n",
    "        \n",
    "            total_loss = pixel_loss + perceptual_adversarial_loss\n",
    "            total_loss.backward()\n",
    "            optimizer_generator.step()\n",
    "            \n",
    "            pbar_epoch.update(1)\n",
    "            pbar_epoch.set_postfix({'Discriminator loss': loss_discriminator.item(), 'Generator loss': total_loss.item() })\n",
    "            \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"[Epoch {epoch}/{num_epochs}] [D loss: {loss_discriminator.item():.4f}] [G loss: {total_loss.item():.4f}] \"\n",
    "              f\"[Pixel loss: {pixel_loss.item():.4f}] [VGG loss: {perceptual_loss.item():.4f}] \"\n",
    "              f\"[Adversarial loss: {adversarial_loss.item():.4f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
