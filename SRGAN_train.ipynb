{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.models import vgg19\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 50\n",
    "batch_size = 16\n",
    "scale = 4\n",
    "patch_size = (96, 96)\n",
    "num_residual_blocks = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_patches(image, patch_size):\n",
    "    channels, height, width = image.size()\n",
    "    patch_height, patch_width = patch_size\n",
    "    patches = []\n",
    "    \n",
    "    # Calculate padding needed for width and height\n",
    "    pad_height = patch_height - height%patch_height\n",
    "    pad_width = patch_width - width%patch_width\n",
    "    \n",
    "    # Apply the padding\n",
    "    padded_image = F.pad(image, (0, pad_width, 0, pad_height))\n",
    "    \n",
    "    # start, stop, step\n",
    "    for y in range(0, height, patch_height):\n",
    "        for x in range(0, width, patch_width):\n",
    "            # the patch consist of all the channels, \n",
    "            patch = padded_image[:, y:y+patch_height, x:x+patch_width]\n",
    "            patches.append(patch)\n",
    "            \n",
    "            \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images_in_folder(folder_path, scale, patch_size):\n",
    "    \n",
    "    processed_images = []\n",
    "    \n",
    "    for filename in tqdm(os.listdir(folder_path), desc=\"Processing images\", colour='green') :\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        input_image = cv2.imread(image_path)\n",
    "        \n",
    "        input_image_rgb = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # change cv2 BGR to RGB\n",
    "        GT_tensor = torch.tensor(input_image_rgb, dtype=torch.float32).permute(2,0,1)/255 * 2 -1\n",
    "        \n",
    "        \n",
    "        GT_image_pil = Image.fromarray(input_image_rgb)\n",
    "        LR_image = TF.resize(GT_image_pil, [GT_image_pil.size[1] // scale, GT_image_pil.size[0] // scale], interpolation=Image.BICUBIC)\n",
    "        \n",
    "        LR_tensor_resized = TF.resize(LR_image, [GT_tensor.shape[1], GT_tensor.shape[2]], interpolation=Image.BICUBIC)\n",
    "        LR_tensor_resized = TF.to_tensor(LR_tensor_resized).float() * 2 - 1\n",
    "        \n",
    "        LR_patches = image_to_patches(LR_tensor_resized, patch_size)\n",
    "        GT_patches = image_to_patches(GT_tensor, patch_size)\n",
    "        processed_images.extend(zip(LR_patches, GT_patches))\n",
    "        \n",
    "    return processed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(LR, GT):\n",
    "    \n",
    "    # Display Ground Truth Image\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(GT.permute(1,2,0))\n",
    "    plt.title(\"Ground Truth (GT)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Display the input image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(LR.permute(1, 2, 0))\n",
    "    plt.title(\"Low Resolution (LR)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing training and test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|\u001b[32m██████████\u001b[0m| 100/100 [00:01<00:00, 52.89it/s]\n",
      "Processing images: 100%|\u001b[32m██████████\u001b[0m| 5/5 [00:00<00:00, 60.77it/s]\n",
      "Processing images: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:00<00:00, 55.27it/s]\n"
     ]
    }
   ],
   "source": [
    "train_images_folder_path = 'data/Train'\n",
    "test_images_set5_folder_path = 'data/Test/Set5'\n",
    "test_images_set14_folder_path = 'data/Test/Set14'\n",
    "\n",
    "processed_train_images = preprocess_images_in_folder(train_images_folder_path, scale, patch_size)\n",
    "processed_test_set5_images = preprocess_images_in_folder(test_images_set5_folder_path, scale, patch_size)\n",
    "processed_test_set14_images = preprocess_images_in_folder(test_images_set14_folder_path, scale, patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 96, 96])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming processed_images is your list of LR and GT patches\n",
    "random_pair = random.choice(processed_test_set5_images)\n",
    "LR_patch, GT_patch = random_pair\n",
    "\n",
    "# Plot LR image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(LR_patch.permute(1, 2, 0))  # Permute LR image dimensions to (height, width, channels) and plot\n",
    "plt.title('Low-Resolution (LR) Image')\n",
    "plt.axis('off')\n",
    "\n",
    "print(LR_patch.size())\n",
    "# Plot GT image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(GT_patch.permute(1, 2, 0))  # Assuming GT image is also in (height, width, channels) format\n",
    "plt.title('Ground Truth (GT) Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting images in DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(processed_train_images, batch_size=batch_size)\n",
    "test_set5_loader = DataLoader(processed_test_set5_images, batch_size=batch_size)\n",
    "test_set14_loader = DataLoader(processed_test_set14_images, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Residual Block classes for Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlockGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResBlockGenerator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding='same')\n",
    "        self.batch_norm1 = nn.BatchNorm2d(64, momentum=0.5)\n",
    "        self.p_relu = nn.PReLU()\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding='same')\n",
    "        self.batch_norm2 = nn.BatchNorm2d(64, momentum=0.5)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        out = self.conv1(input_tensor)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.p_relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        out += input_tensor\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_residual_blocks = num_residual_blocks):\n",
    "        super(Generator, self).__init__()\n",
    "        # Pre residual layers\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, stride=1, padding='same')\n",
    "        self.p_relu1 = nn.PReLU()\n",
    "        \n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResBlockGenerator() for _ in range(num_residual_blocks)]\n",
    "        )\n",
    "        \n",
    "        # Post residual layers\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding='same')\n",
    "        self.batch_norm = nn.BatchNorm2d(64, momentum=0.5)\n",
    "        \n",
    "        # Upsampling layers\n",
    "        self.upsample1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        self.upsample2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        # Output convolutional layer\n",
    "        self.conv3 = nn.Conv2d(64, 3, kernel_size=9, stride=1, padding='same')\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        out = self.conv1(input_tensor)\n",
    "        out = self.p_relu1(out)\n",
    "        input_residual_tensor = out.clone()\n",
    "        \n",
    "        for res_block in self.residual_blocks:\n",
    "            out = res_block(out)\n",
    "            \n",
    "        out = self.conv2(out)\n",
    "        out = self.batch_norm(out)\n",
    "        out += input_residual_tensor\n",
    "        \n",
    "        out = self.upsample1(out)\n",
    "        out = self.upsample2(out)\n",
    "            \n",
    "        out = self.conv3(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorBlock(nn.Module):\n",
    "    def __init__(self, in_channel,out_channel, strides = 1, padding_same=True):\n",
    "        super(DiscriminatorBlock, self).__init__()\n",
    "        \n",
    "        if padding_same:\n",
    "            self.padding = 'same'\n",
    "        else:\n",
    "            self.padding = 0\n",
    "            \n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=strides, padding=self.padding)\n",
    "        self.batch_normalization = nn.BatchNorm2d(out_channel, momentum=0.8)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        out = self.conv(input_tensor)\n",
    "        out = self.batch_normalization(out)\n",
    "        out = self.leaky_relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding='same')\n",
    "        self.leaky_relu1 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.block_with_batch_norm = nn.Sequential(\n",
    "            DiscriminatorBlock(64, 64, strides=2, padding_same=False),\n",
    "            DiscriminatorBlock(64, 128),\n",
    "            DiscriminatorBlock(128, 128,strides=2, padding_same=False),\n",
    "            DiscriminatorBlock(128, 256),\n",
    "            DiscriminatorBlock(256, 256, strides=2, padding_same=False),\n",
    "            DiscriminatorBlock(256, 512, ),\n",
    "            DiscriminatorBlock(512, 512, strides=2, padding_same=False)\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(512 * 5 * 5, 1024)\n",
    "        self.leaky_relu2 = nn.LeakyReLU(0.2)\n",
    "        self.dense2 = nn.Linear(1024, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        out = self.conv1(input_tensor)\n",
    "        out = self.leaky_relu1(out)\n",
    "        out = self.block_with_batch_norm(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.dense1(out)\n",
    "        out = self.leaky_relu2(out)\n",
    "        out = self.dense2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating VGG loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phi_2,2, after 2nd activation, but before the 2nd max pooling\n",
    "\n",
    "class VGG19Loss(nn.Module):\n",
    "    def __init__(self, layer_index=9):\n",
    "        super(VGG19Loss, self).__init__()\n",
    "        vgg19_model = vgg19(weights='DEFAULT')\n",
    "        \n",
    "        # vgg19_model.features.children() returns an iterator over each individual layer\n",
    "        # list() converts the iterator into a Python list\n",
    "        # [:layer_index + 1] slices the list up to and including the layer at layer_index\n",
    "        # * is the unpacking operator, which makes the list elements suitable to be passed to nn.Sequential \n",
    "        self.features = nn.Sequential(*list(vgg19_model.features.children())[:layer_index + 1])\n",
    "        \n",
    "        # Make sure no updating of the parameters occur\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, sr, hr):\n",
    "        sr_features = self.features(sr)\n",
    "        hr_features = self.features(hr)\n",
    "        \n",
    "        loss = self.criterion(sr_features, hr_features)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining generator, discriminator and VGG loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG19Loss(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (criterion): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = Generator(num_residual_blocks=16)\n",
    "discriminator = Discriminator()\n",
    "vgg_loss = VGG19Loss()\n",
    "\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "vgg_loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining optimizers and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "optimizer_generator = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=5e-5, betas=(0.9, 0.999))\n",
    "\n",
    "scheduler_generator = ReduceLROnPlateau(optimizer_generator, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "scheduler_discriminator = ReduceLROnPlateau(optimizer_discriminator, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "adversarial_criterion = nn.BCELoss()\n",
    "pixel_criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 125/125 [34:05<00:00, 16.37s/it, Discriminator loss=7.33, Generator loss=3.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/50] [D loss: 7.3321] [G loss: 3.0282] [Pixel loss: 0.1631] [VGG loss: 2.8635] [Adversarial loss: 1.6097]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 125/125 [34:20<00:00, 16.48s/it, Discriminator loss=8.86, Generator loss=2.84] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/50] [D loss: 8.8583] [G loss: 2.8438] [Pixel loss: 0.1217] [VGG loss: 2.7166] [Adversarial loss: 5.5346]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 125/125 [33:55<00:00, 16.29s/it, Discriminator loss=8.12, Generator loss=2.74] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/50] [D loss: 8.1225] [G loss: 2.7448] [Pixel loss: 0.0941] [VGG loss: 2.6469] [Adversarial loss: 3.8384]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 125/125 [35:08<00:00, 16.87s/it, Discriminator loss=29.4, Generator loss=2.76] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/50] [D loss: 29.3537] [G loss: 2.7613] [Pixel loss: 0.0733] [VGG loss: 2.6297] [Adversarial loss: 58.2288]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 125/125 [34:25<00:00, 16.52s/it, Discriminator loss=50, Generator loss=2.71]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5/50] [D loss: 50.0000] [G loss: 2.7065] [Pixel loss: 0.0632] [VGG loss: 2.5933] [Adversarial loss: 50.0000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 125/125 [35:35<00:00, 17.09s/it, Discriminator loss=50, Generator loss=2.61]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6/50] [D loss: 50.0000] [G loss: 2.6111] [Pixel loss: 0.0558] [VGG loss: 2.5116] [Adversarial loss: 43.7500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 125/125 [34:43<00:00, 16.67s/it, Discriminator loss=50, Generator loss=2.57]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7/50] [D loss: 50.0000] [G loss: 2.5687] [Pixel loss: 0.0482] [VGG loss: 2.4767] [Adversarial loss: 43.7500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 125/125 [34:24<00:00, 16.51s/it, Discriminator loss=50, Generator loss=2.5]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8/50] [D loss: 50.0076] [G loss: 2.5038] [Pixel loss: 0.0436] [VGG loss: 2.4214] [Adversarial loss: 38.8248]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 125/125 [34:30<00:00, 16.56s/it, Discriminator loss=50, Generator loss=2.47]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9/50] [D loss: 50.0000] [G loss: 2.4739] [Pixel loss: 0.0399] [VGG loss: 2.3903] [Adversarial loss: 43.7500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 125/125 [34:52<00:00, 16.74s/it, Discriminator loss=53.1, Generator loss=2.47] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/50] [D loss: 53.1250] [G loss: 2.4699] [Pixel loss: 0.0391] [VGG loss: 2.3933] [Adversarial loss: 37.5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 125/125 [33:46<00:00, 16.21s/it, Discriminator loss=50, Generator loss=2.43]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11/50] [D loss: 50.0000] [G loss: 2.4296] [Pixel loss: 0.0356] [VGG loss: 2.3502] [Adversarial loss: 43.7500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 125/125 [33:37<00:00, 16.14s/it, Discriminator loss=50, Generator loss=2.39]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12/50] [D loss: 50.0000] [G loss: 2.3892] [Pixel loss: 0.0341] [VGG loss: 2.3176] [Adversarial loss: 37.5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 125/125 [35:10<00:00, 16.89s/it, Discriminator loss=46.9, Generator loss=2.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13/50] [D loss: 46.8750] [G loss: 2.3721] [Pixel loss: 0.0326] [VGG loss: 2.3020] [Adversarial loss: 37.5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 125/125 [33:15<00:00, 15.96s/it, Discriminator loss=46.9, Generator loss=2.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14/50] [D loss: 46.8750] [G loss: 2.3310] [Pixel loss: 0.0316] [VGG loss: 2.2619] [Adversarial loss: 37.5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 125/125 [33:39<00:00, 16.16s/it, Discriminator loss=48.2, Generator loss=2.3]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15/50] [D loss: 48.2185] [G loss: 2.2967] [Pixel loss: 0.0281] [VGG loss: 2.2566] [Adversarial loss: 12.1151]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 125/125 [33:34<00:00, 16.12s/it, Discriminator loss=50, Generator loss=2.28]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16/50] [D loss: 50.0000] [G loss: 2.2815] [Pixel loss: 0.0275] [VGG loss: 2.2540] [Adversarial loss: 0.0000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 125/125 [33:11<00:00, 15.94s/it, Discriminator loss=50, Generator loss=2.26]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17/50] [D loss: 50.0000] [G loss: 2.2561] [Pixel loss: 0.0275] [VGG loss: 2.2286] [Adversarial loss: 0.0000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 125/125 [33:07<00:00, 15.90s/it, Discriminator loss=50, Generator loss=2.26]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18/50] [D loss: 50.0000] [G loss: 2.2563] [Pixel loss: 0.0276] [VGG loss: 2.2287] [Adversarial loss: 0.0000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 125/125 [33:07<00:00, 15.90s/it, Discriminator loss=50, Generator loss=2.22]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19/50] [D loss: 50.0000] [G loss: 2.2230] [Pixel loss: 0.0276] [VGG loss: 2.1954] [Adversarial loss: 0.0000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 125/125 [33:10<00:00, 15.92s/it, Discriminator loss=50, Generator loss=2.22]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20/50] [D loss: 50.0000] [G loss: 2.2188] [Pixel loss: 0.0274] [VGG loss: 2.1913] [Adversarial loss: 0.0000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 125/125 [33:29<00:00, 16.08s/it, Discriminator loss=50, Generator loss=2.22]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21/50] [D loss: 50.0000] [G loss: 2.2153] [Pixel loss: 0.0272] [VGG loss: 2.1881] [Adversarial loss: 0.0000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 125/125 [33:59<00:00, 16.32s/it, Discriminator loss=50, Generator loss=2.2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22/50] [D loss: 50.0000] [G loss: 2.1994] [Pixel loss: 0.0259] [VGG loss: 2.1736] [Adversarial loss: 0.0000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 125/125 [34:15<00:00, 16.44s/it, Discriminator loss=50, Generator loss=2.19]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23/50] [D loss: 50.0000] [G loss: 2.1914] [Pixel loss: 0.0261] [VGG loss: 2.1652] [Adversarial loss: 0.0000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50:  16%|█▌        | 20/125 [05:34<29:14, 16.71s/it, Discriminator loss=50, Generator loss=1.94] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 35\u001b[0m\n\u001b[0;32m     31\u001b[0m adversarial_loss \u001b[38;5;241m=\u001b[39m adversarial_criterion(outputs_fake, real_labels)\n\u001b[0;32m     33\u001b[0m pixel_loss \u001b[38;5;241m=\u001b[39m pixel_criterion(fake_hr_resized, imgs_hr)\n\u001b[1;32m---> 35\u001b[0m perceptual_loss \u001b[38;5;241m=\u001b[39m \u001b[43mvgg_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_hr_resized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs_hr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m perceptual_adversarial_loss \u001b[38;5;241m=\u001b[39m perceptual_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.001\u001b[39m \u001b[38;5;241m*\u001b[39m adversarial_loss\n\u001b[0;32m     39\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m pixel_loss \u001b[38;5;241m+\u001b[39m perceptual_adversarial_loss\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 22\u001b[0m, in \u001b[0;36mVGG19Loss.forward\u001b[1;34m(self, sr, hr)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sr, hr):\n\u001b[0;32m     21\u001b[0m     sr_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures(sr)\n\u001b[1;32m---> 22\u001b[0m     hr_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(sr_features, hr_features)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:164\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:796\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    795\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}', position=0, leave=True) as pbar_epoch:\n",
    "        for i, (imgs_lr, imgs_hr) in enumerate(train_loader):\n",
    "            imgs_lr = imgs_lr.to(device)\n",
    "            imgs_hr = imgs_hr.to(device)\n",
    "        \n",
    "            # Training the generator\n",
    "            fake_hr = generator(imgs_lr)\n",
    "            fake_hr_resized = F.interpolate(fake_hr, size=(96, 96), mode='bilinear', align_corners=False)\n",
    "        \n",
    "            optimizer_generator.zero_grad()\n",
    "        \n",
    "            real_labels = torch.ones((imgs_hr.size(0), 1), requires_grad=False).to(device)\n",
    "            fake_labels = torch.zeros((imgs_hr.size(0), 1), requires_grad=False).to(device)\n",
    "        \n",
    "            # Training the discriminator with real images\n",
    "            outputs_real = discriminator(imgs_hr)\n",
    "            loss_real = adversarial_criterion(outputs_real, real_labels)\n",
    "        \n",
    "            # Training discriminator with fake images\n",
    "            outputs_fake = discriminator(fake_hr_resized.detach())\n",
    "            loss_fake = adversarial_criterion(outputs_fake, fake_labels)\n",
    "        \n",
    "            loss_discriminator = (loss_real + loss_fake)/2\n",
    "            loss_discriminator.backward()\n",
    "            optimizer_discriminator.step()\n",
    "        \n",
    "            optimizer_generator.zero_grad()\n",
    "        \n",
    "            outputs_fake = discriminator(fake_hr_resized)\n",
    "            adversarial_loss = adversarial_criterion(outputs_fake, real_labels)\n",
    "        \n",
    "            pixel_loss = pixel_criterion(fake_hr_resized, imgs_hr)\n",
    "        \n",
    "            perceptual_loss = vgg_loss(fake_hr_resized, imgs_hr)\n",
    "        \n",
    "            perceptual_adversarial_loss = perceptual_loss + 0.001 * adversarial_loss\n",
    "        \n",
    "            total_loss = pixel_loss + perceptual_adversarial_loss\n",
    "            total_loss.backward()\n",
    "            optimizer_generator.step()\n",
    "            \n",
    "            pbar_epoch.update(1)\n",
    "            pbar_epoch.set_postfix({'Discriminator loss': loss_discriminator.item(), 'Generator loss': total_loss.item()})\n",
    "            \n",
    "        scheduler_generator.step(total_loss)\n",
    "        scheduler_discriminator.step(loss_discriminator)\n",
    "            \n",
    "    print(f\"[Epoch {epoch+1}/{num_epochs}] [D loss: {loss_discriminator.item():.4f}] [G loss: {total_loss.item():.4f}] \"\n",
    "              f\"[Pixel loss: {pixel_loss.item():.4f}] [VGG loss: {perceptual_loss.item():.4f}] \"\n",
    "              f\"[Adversarial loss: {adversarial_loss.item():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop with num of discriminator update per generator update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/10:   0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "n_critic = 3  # Number of discriminator updates per generator update\n",
    "num_epochs = 10\n",
    "clip_value = 0.01  # For gradient clipping (optional)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch}/{num_epochs}', position=0, leave=True) as pbar_epoch:\n",
    "        for i, (imgs_lr, imgs_hr) in enumerate(train_loader):\n",
    "            imgs_lr = (imgs_lr / 255.0) * 2 - 1  # Normalize to [-1, 1]\n",
    "            imgs_hr = (imgs_hr / 255.0) * 2 - 1  # Normalize to [-1, 1]\n",
    "            \n",
    "            imgs_lr = imgs_lr.to(device)\n",
    "            imgs_hr = imgs_hr.to(device)\n",
    "\n",
    "            # Update Discriminator n_critic times\n",
    "            for _ in range(n_critic):\n",
    "                optimizer_discriminator.zero_grad()\n",
    "\n",
    "                # Training the discriminator with real images\n",
    "                outputs_real = discriminator(imgs_hr)\n",
    "                real_labels = torch.ones((imgs_hr.size(0), 1), requires_grad=False).to(device)\n",
    "                fake_labels = torch.zeros((imgs_hr.size(0), 1), requires_grad=False).to(device)\n",
    "                loss_real = adversarial_criterion(outputs_real, real_labels)\n",
    "\n",
    "                # Training discriminator with fake images\n",
    "                fake_hr = generator(imgs_lr).detach()\n",
    "                fake_hr_resized = F.interpolate(fake_hr, size=(96, 96), mode='bilinear', align_corners=False)\n",
    "                outputs_fake = discriminator(fake_hr_resized)\n",
    "                loss_fake = adversarial_criterion(outputs_fake, fake_labels)\n",
    "\n",
    "                loss_discriminator = (loss_real + loss_fake) / 2\n",
    "                loss_discriminator.backward()\n",
    "                \n",
    "                # Gradient Clipping (optional)\n",
    "                if clip_value:\n",
    "                    torch.nn.utils.clip_grad_norm_(discriminator.parameters(), clip_value)\n",
    "                \n",
    "                optimizer_discriminator.step()\n",
    "        \n",
    "            # Update Generator\n",
    "            optimizer_generator.zero_grad()\n",
    "            fake_hr = generator(imgs_lr)\n",
    "            fake_hr_resized = F.interpolate(fake_hr, size=(96, 96), mode='bilinear', align_corners=False)\n",
    "            outputs_fake = discriminator(fake_hr_resized)\n",
    "            adversarial_loss = adversarial_criterion(outputs_fake, real_labels)\n",
    "\n",
    "            pixel_loss = pixel_criterion(fake_hr_resized, imgs_hr)\n",
    "            perceptual_loss = vgg_loss(fake_hr_resized, imgs_hr)\n",
    "            perceptual_adversarial_loss = perceptual_loss + 0.001 * adversarial_loss\n",
    "\n",
    "            total_loss = pixel_loss + perceptual_adversarial_loss\n",
    "            total_loss.backward()\n",
    "            optimizer_generator.step()\n",
    "            \n",
    "            pbar_epoch.update(1)\n",
    "            pbar_epoch.set_postfix({'Discriminator loss': loss_discriminator.item(), 'Generator loss': total_loss.item()})\n",
    "            \n",
    "        scheduler_generator.step(total_loss)\n",
    "        scheduler_discriminator.step(loss_discriminator)\n",
    "            \n",
    "    print(f\"[Epoch {epoch+1}/{num_epochs}] [D loss: {loss_discriminator.item():.4f}] [G loss: {total_loss.item():.4f}] \"\n",
    "              f\"[Pixel loss: {pixel_loss.item():.4f}] [VGG loss: {perceptual_loss.item():.4f}] \"\n",
    "              f\"[Adversarial loss: {adversarial_loss.item():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_best = generator.state_dict()\n",
    "generator.load_state_dict(generator_best)\n",
    "torch.save(generator.state_dict(), 'SRGAN.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: results/Set14\\baboon.bmp\n",
      "Processed and saved: results/Set14\\barbara.bmp\n",
      "Processed and saved: results/Set14\\bridge.bmp\n",
      "Processed and saved: results/Set14\\coastguard.bmp\n",
      "Processed and saved: results/Set14\\comic.bmp\n",
      "Processed and saved: results/Set14\\face.bmp\n",
      "Processed and saved: results/Set14\\flowers.bmp\n",
      "Processed and saved: results/Set14\\foreman.bmp\n",
      "Processed and saved: results/Set14\\lenna.bmp\n",
      "Processed and saved: results/Set14\\man.bmp\n",
      "Processed and saved: results/Set14\\monarch.bmp\n",
      "Processed and saved: results/Set14\\pepper.bmp\n",
      "Processed and saved: results/Set14\\ppt3.bmp\n",
      "Processed and saved: results/Set14\\zebra.bmp\n"
     ]
    }
   ],
   "source": [
    "# Function to process and save image\n",
    "def process_and_save_image(input_image_path, output_image_path):\n",
    "    input_image = cv2.imread(input_image_path)  # Read input image\n",
    "\n",
    "    # Convert from BGR to RGB\n",
    "    input_image_rgb = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "    # Convert image to tensor and normalize to [-1. 1]\n",
    "    input_tensor = torch.tensor(input_image_rgb, dtype=torch.float32).permute(2, 0, 1) / 255.0 * 2 - 1\n",
    "    \n",
    "    # Add batch dimension\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        SR_tensor = generator(input_tensor, False)\n",
    "\n",
    "    # Post-process SR tensor\n",
    "    SR_tensor = SR_tensor.squeeze().permute(1, 2, 0).cpu().numpy()  # Remove batch dimension, permute to HWC\n",
    "    SR_tensor = np.clip((SR_tensor + 1) * 255.0 / 2, 0, 255).astype(np.uint8)  # Rescale to [0, 255] and convert to uint8\n",
    "\n",
    "    # Resize the output image by reducing it to one-fourth of its original dimensions\n",
    "    SR_tensor_resized = cv2.resize(SR_tensor, (SR_tensor.shape[1] // 4, SR_tensor.shape[0] // 4), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Save the super-resolved image\n",
    "    cv2.imwrite(output_image_path, cv2.cvtColor(SR_tensor_resized, cv2.COLOR_RGB2BGR))\n",
    "    print(f\"Processed and saved: {output_image_path}\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "input_dir = 'data/Test/Set14'\n",
    "output_dir = 'results/Set14'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process all images in the specified input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.bmp'):\n",
    "        input_image_path = os.path.join(input_dir, filename)\n",
    "        output_image_path = os.path.join(output_dir, filename)\n",
    "        process_and_save_image(input_image_path, output_image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
